[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Business Intelligence Real Estate Analytics Tool",
    "section": "",
    "text": "Welcome to my final project website.\nThis project includes:\n\nGeospatial preprocessing\n\nParcel-level redevelopment opportunity scoring\n\nPredictive modeling\n\nInteractive dashboards\n\nOptimization using PuLP\n\n\n\n\n\nOpen the Analysis Notebook"
  },
  {
    "objectID": "index.html#full-jupyter-notebook-test",
    "href": "index.html#full-jupyter-notebook-test",
    "title": "A Business Intelligence Real Estate Analytics Tool",
    "section": "",
    "text": "Open the Analysis Notebook"
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "Development Opportunity Optimizer",
    "section": "",
    "text": "Unlocking Under-Utilized Urban Property for Real-Estate Development\nMUSA 5500 Geospatial Data Science in Pytho\nUniversity of Pennslyvania\nFinal Project\nAuthor: Joshua Rigsby\nInstructor: Dr.¬†Xiojiang Li\n\n\n1. Introduction\nThis project is predicated by real estate and financial research to design and build an computational analytical tool that assists the development team during the feasability phase of real estate development.\nResearch Question\nWhich properties, (vacant and developed) in Philadelphia present the highest redevelopment potential determined by: market value uplift, market gap, land utilization, accesability, and zoning capacity?\nObjectives\nThe research question is studied and analyzied through the design and implementation of a geospatial data model that scores and ranks properties according to redevelopment potential. The result is the creation of a data-driven highest and best use development screening tool, that significantly aids development through the automation of market research, site selection, and value projection at a large scale, saving time and resources during idea inception, refinement and feasibility. Given that the data exists the model can be easily refitted to accomodate any city in the world.\nThe model will identify properties and parcels in Philadelphia, that are under-utilized, vacant, low value, and maintain old zoning, to then optimize a development scenario, via - proposing best use (residential, mixed-use, infill), - estimating value uplift - prioritizing optimization of opportunity.\nThe output is both a data-science and business development tool usable by a developement and investment teams.\n\n\n2. Methodology\nData Identification\nData on building ansd zoning permits, real estate transfers, street networks via OSMnx, Philadelphia property assesments, vacant property indicators, zoning overlays, census data, and several gegraphic boundries has been pre selected to run the model\nData Reading and combining:\nThe data will be read into GeoPandas DataFrames with a common CRS and matching parcel identifiers.\nSpatial Joining\nData will be aggregated and joined to parcel data serving as the initial link between geographic data and numerical data.\nFinancial and Land Computations\nCalculations for build ratio proxy to represent underutilization, market gap calculations, flags for old structures, accessibility score calculations, zoning capacity calculations, potential uplift value calculations.\nFinancial Optimization\nEstimation of potential project value for each parcel using zoning capacity and industry standard pro-forma assumptions.\nRedevelopment Opportunity Score: Financial + Spatial\nAll calculations culminate with the computaion for an opportunity Score, using a weighted composite of metrics, using weighted metrics that and assign weights reflecting business priorities.\nVisual Dashboard\n\nDisplays properties colored by opportunity score\nSlider to adjust budget and re-run optimization\nTable of top 10 parcels\nHover to show key financial computations\n\n\n\nAnalysis\n\n\n3. Data onboarding, Data Preparation and Feature Engineering\nMaybe add subtitle still need to do feature engineering maybe that can be a seperate section\n\n1.1 Python Package Imports\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport osmnx as ox\nimport networkx as nx\nimport hvplot.pandas \nimport holoviews as hv\nimport folium\nimport datashader as ds\nimport xyzservices\nimport seaborn as sns\nimport pulp\nimport altair as alt\nalt.data_transformers.enable(\"vegafusion\") \nimport matplotlib.pyplot as plt\nimport panel as pn\npn.extension()\n\nfrom shapely.geometry import Point\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.spatial import cKDTree\nfrom pulp import LpMaximize, LpProblem, LpVariable, lpSum\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.2 Establish Universal Coordinate Reference System for Philadelphia\nadd text here explining what this is\n\nphl_crs = 2272  # EPSG:2272\n\n\n\n1.3 Data Overview\nGraphic explaining data\ngeojson for gep data csv for tabular records\nProperty Assesments: ‚Äúparcel geometry, land use, assessed land/building value, year built, lot size‚Äù\n\n\n1.4 Data Onboarding\nEach data set is read in via geopandas data frames and converted to a Philadelphia coordinate refernce syetem.\n\n# 1.1 Property Assessments (Parcels)\nproperty_assessment_info = gpd.read_file(\n    \"data/opa_properties_public.geojson\" \n).to_crs(phl_crs)\n\n# 1.2 Vacant Property Indicators\nvacant_property_indicators = gpd.read_file(\n    \"data/Vacant_Indicators_Land.geojson\"  \n).to_crs(phl_crs)\n\n# 1.3 Zoning Base Districts\nzoning_districts = gpd.read_file(\n    \"data/Zoning_BaseDistricts.geojson\"  \n).to_crs(phl_crs)\n\n# 1.4 Zoning Overlays\nzoning_overlays = gpd.read_file(\n    \"data/Zoning_Overlays.geojson\"  \n).to_crs(phl_crs)\n\n# 1.5 Permits (Building & Zoning)\npermit_data = pd.read_csv(\n    \"/Users/JoshuaRigsby 1/Desktop/permits.csv\")\n\n# 1.6 Real estate transfers (Sales)\nsales_transfers_data = pd.read_csv(\n    \"data/RTT_SUMMARY.csv\"  \n)\n\n# 1.6 (Optional) ACS data\n# acs = cny.products.APIConnection(\"ACSDT5Y2022\").query_variables([...])\n# ... etc, only if you decide to add it.\n\n\n---------------------------------------------------------------------------\nDataSourceError                           Traceback (most recent call last)\nCell In[55], line 2\n      1 # 1.1 Property Assessments (Parcels)\n----&gt; 2 property_assessment_info = gpd.read_file(\n      3     \"data/opa_properties_public.geojson\" \n      4 ).to_crs(phl_crs)\n      6 # 1.2 Vacant Property Indicators\n      7 vacant_property_indicators = gpd.read_file(\n      8     \"data/Vacant_Indicators_Land.geojson\"  \n      9 ).to_crs(phl_crs)\n\nFile /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/geopandas/io/file.py:316, in _read_file(filename, bbox, mask, columns, rows, engine, **kwargs)\n    313             filename = response.read()\n    315 if engine == \"pyogrio\":\n--&gt; 316     return _read_file_pyogrio(\n    317         filename, bbox=bbox, mask=mask, columns=columns, rows=rows, **kwargs\n    318     )\n    320 elif engine == \"fiona\":\n    321     if pd.api.types.is_file_like(filename):\n\nFile /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/geopandas/io/file.py:576, in _read_file_pyogrio(path_or_bytes, bbox, mask, rows, **kwargs)\n    567     warnings.warn(\n    568         \"The 'include_fields' and 'ignore_fields' keywords are deprecated, and \"\n    569         \"will be removed in a future release. You can use the 'columns' keyword \"\n   (...)    572         stacklevel=3,\n    573     )\n    574     kwargs[\"columns\"] = kwargs.pop(\"include_fields\")\n--&gt; 576 return pyogrio.read_dataframe(path_or_bytes, bbox=bbox, **kwargs)\n\nFile /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/pyogrio/geopandas.py:275, in read_dataframe(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\n    270 if not use_arrow:\n    271     # For arrow, datetimes are read as is.\n    272     # For numpy IO, datetimes are read as string values to preserve timezone info\n    273     # as numpy does not directly support timezones.\n    274     kwargs[\"datetime_as_string\"] = True\n--&gt; 275 result = read_func(\n    276     path_or_buffer,\n    277     layer=layer,\n    278     encoding=encoding,\n    279     columns=columns,\n    280     read_geometry=read_geometry,\n    281     force_2d=gdal_force_2d,\n    282     skip_features=skip_features,\n    283     max_features=max_features,\n    284     where=where,\n    285     bbox=bbox,\n    286     mask=mask,\n    287     fids=fids,\n    288     sql=sql,\n    289     sql_dialect=sql_dialect,\n    290     return_fids=fid_as_index,\n    291     **kwargs,\n    292 )\n    294 if use_arrow:\n    295     import pyarrow as pa\n\nFile /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/pyogrio/raw.py:198, in read(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\n     59 \"\"\"Read OGR data source into numpy arrays.\n     60 \n     61 IMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\n   (...)    194 \n    195 \"\"\"\n    196 dataset_kwargs = _preprocess_options_key_value(kwargs) if kwargs else {}\n--&gt; 198 return ogr_read(\n    199     get_vsi_path_or_buffer(path_or_buffer),\n    200     layer=layer,\n    201     encoding=encoding,\n    202     columns=columns,\n    203     read_geometry=read_geometry,\n    204     force_2d=force_2d,\n    205     skip_features=skip_features,\n    206     max_features=max_features or 0,\n    207     where=where,\n    208     bbox=bbox,\n    209     mask=_mask_to_wkb(mask),\n    210     fids=fids,\n    211     sql=sql,\n    212     sql_dialect=sql_dialect,\n    213     return_fids=return_fids,\n    214     dataset_kwargs=dataset_kwargs,\n    215     datetime_as_string=datetime_as_string,\n    216 )\n\nFile pyogrio/_io.pyx:1313, in pyogrio._io.ogr_read()\n\nFile pyogrio/_io.pyx:232, in pyogrio._io.ogr_open()\n\nDataSourceError: data/opa_properties_public.geojson: No such file or directory\n\n\n\n\n\n1.4.2 Data Onboarding - Parcels via seperate pipeline for efficiency\n\n# 1.7 Parcel Geometry\nparcel_geometry = gpd.read_file(\n    \"data/DOR_Parcel.geojson\"  \n).to_crs(phl_crs)\n\n\n---------------------------------------------------------------------------\nDataSourceError                           Traceback (most recent call last)\nCell In[56], line 2\n      1 # 1.7 Parcel Geometry\n----&gt; 2 parcel_geometry = gpd.read_file(\n      3     \"data/DOR_Parcel.geojson\"  \n      4 ).to_crs(phl_crs)\n\nFile /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/geopandas/io/file.py:316, in _read_file(filename, bbox, mask, columns, rows, engine, **kwargs)\n    313             filename = response.read()\n    315 if engine == \"pyogrio\":\n--&gt; 316     return _read_file_pyogrio(\n    317         filename, bbox=bbox, mask=mask, columns=columns, rows=rows, **kwargs\n    318     )\n    320 elif engine == \"fiona\":\n    321     if pd.api.types.is_file_like(filename):\n\nFile /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/geopandas/io/file.py:576, in _read_file_pyogrio(path_or_bytes, bbox, mask, rows, **kwargs)\n    567     warnings.warn(\n    568         \"The 'include_fields' and 'ignore_fields' keywords are deprecated, and \"\n    569         \"will be removed in a future release. You can use the 'columns' keyword \"\n   (...)    572         stacklevel=3,\n    573     )\n    574     kwargs[\"columns\"] = kwargs.pop(\"include_fields\")\n--&gt; 576 return pyogrio.read_dataframe(path_or_bytes, bbox=bbox, **kwargs)\n\nFile /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/pyogrio/geopandas.py:275, in read_dataframe(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\n    270 if not use_arrow:\n    271     # For arrow, datetimes are read as is.\n    272     # For numpy IO, datetimes are read as string values to preserve timezone info\n    273     # as numpy does not directly support timezones.\n    274     kwargs[\"datetime_as_string\"] = True\n--&gt; 275 result = read_func(\n    276     path_or_buffer,\n    277     layer=layer,\n    278     encoding=encoding,\n    279     columns=columns,\n    280     read_geometry=read_geometry,\n    281     force_2d=gdal_force_2d,\n    282     skip_features=skip_features,\n    283     max_features=max_features,\n    284     where=where,\n    285     bbox=bbox,\n    286     mask=mask,\n    287     fids=fids,\n    288     sql=sql,\n    289     sql_dialect=sql_dialect,\n    290     return_fids=fid_as_index,\n    291     **kwargs,\n    292 )\n    294 if use_arrow:\n    295     import pyarrow as pa\n\nFile /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/pyogrio/raw.py:198, in read(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\n     59 \"\"\"Read OGR data source into numpy arrays.\n     60 \n     61 IMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\n   (...)    194 \n    195 \"\"\"\n    196 dataset_kwargs = _preprocess_options_key_value(kwargs) if kwargs else {}\n--&gt; 198 return ogr_read(\n    199     get_vsi_path_or_buffer(path_or_buffer),\n    200     layer=layer,\n    201     encoding=encoding,\n    202     columns=columns,\n    203     read_geometry=read_geometry,\n    204     force_2d=force_2d,\n    205     skip_features=skip_features,\n    206     max_features=max_features or 0,\n    207     where=where,\n    208     bbox=bbox,\n    209     mask=_mask_to_wkb(mask),\n    210     fids=fids,\n    211     sql=sql,\n    212     sql_dialect=sql_dialect,\n    213     return_fids=return_fids,\n    214     dataset_kwargs=dataset_kwargs,\n    215     datetime_as_string=datetime_as_string,\n    216 )\n\nFile pyogrio/_io.pyx:1313, in pyogrio._io.ogr_read()\n\nFile pyogrio/_io.pyx:232, in pyogrio._io.ogr_open()\n\nDataSourceError: data/DOR_Parcel.geojson: No such file or directory\n\n\n\n\n#parcel_geometry.head()\n\n\n#property_assessment_info.head()\n\n\n#vacant_property_indicators.head()\n\n\n#zoning_districts.head()\n\n\n#zoning_overlays.head\n\n\n#permit_data.head()\n\n\n#sales_transfers_data.head()\n\n\n\n1.5 Data Processing and Preparation\nEach dataset contains different id‚Äôs and geometries, so the data underwent a normalization into a common baseline to prepare the data to joined, analyzied and computed.\nData Processing by Data Set\nProperty Assessments\nThe pin variable was renamed to PARCEL_ID for consistency across datasets, market_value, total_livable_area, and year_built were renamed to standardized variable names to be used in feature engineering for computing value gap, build ratio, and depreciation. A tag was added to track the dataset origin. The data was reprojected to EPSG:2272 as a redundant but safe measure. Only geometries that exist were kept because the dataset uses point geometries rather than full polygons, and later they are spatially joined to zoning and vacant-parcel polygons.\nVacant Property Indicators\nThe opa_id variable was renamed to PARCEL_ID for consistency across datasets. A VACANT_FLAG = 1 column was added for filtering. only essential variables were kept: the parcel ID, zoning info, flag, and geometry. The data was reprojected to EPSG:2272 as a redundant but safe measure.\nZoning Base Districts\nColumn names were simplified so so ZONE_TYPE can be analysed more efficiently. Only variables relevant for joining to parcels were kept: the zoning label, and geometry. The data was reprojected to EPSG:2272 as a redundant but safe measure.\nZoning Overlays\nThe overlay_symbol variable was renamed to OVERLAY_TYPE for consistenty, only theoverlay ID and geometry, were kept as most of the overlay attributes are legal text not needed. The data was reprojected to EPSG:2272 as a redundant but safe measure.\nBuilding and Zoning Permits\nPermit id‚Äôs and types were standardized and renamed into concise variable names, only columns useful for tracking recent or active redevelopments were kept. Duplicates were erased to ensure one record per permit.\nReal Estate Transfers\nSale date and price were converted to numeric types. All zero or null sales were removed. Only the most recent sale for each parcel was kept\nOverview\n\nEvery dataset has consistent IDs :PARCEL_ID\nAll geometries share the same coordinate refernce system (EPSG:2272).\nIrrelevant or redundant columns removed.\nData types are properly formatted for numeric and date operations.\nEach dataset is prepared for a clear role in the modeling operation\n\nproperty_assessment_info_cleaned: Financial & physical base data\nvacant_property_indicators_cleaned: Redevelopment candidate footprints\nzoning_districts_cleaned and zoning_overlays_cleaned: Regulatory context\npermit_data_cleaned: Project filter\nsales_transfers_data_cleaned: Market value benchmark\n\n# Property Assessments (Parcels)\nproperty_assessment_info_cleaned = (\n    property_assessment_info.rename(columns={   \n        \"pin\": \"PARCEL_ID\",\n        \"total_livable_area\": \"BUILDING_SQFT\",\n        \"market_value\": \"ASSESSED_VALUE\",\n        \"year_built\": \"YEAR_BUILT\"\n    })\n    .assign(SOURCE=\"Assessments\")\n    .to_crs(phl_crs)\n)\n\nproperty_assessment_info_cleaned = property_assessment_info_cleaned[\n    property_assessment_info_cleaned.geometry.notna()\n]\n\n\n# Vacant Property Indicators\nvacant_property_indicators_cleaned = (\n    vacant_property_indicators.rename(columns={       \n        \"opa_id\": \"PARCEL_ID\",\n        \"zoningbasedistrict\": \"ZONE_BASE\"\n    })\n    .assign(VACANT_FLAG=1)\n)[[\"PARCEL_ID\", \"ZONE_BASE\", \"VACANT_FLAG\", \"geometry\"]].to_crs(phl_crs)\n\n\n# Zoning Base Districts\nzoning_districts_cleaned = zoning_districts.rename(columns={\"code\": \"ZONE_TYPE\"})[\n    [\"ZONE_TYPE\", \"geometry\"]\n].to_crs(phl_crs)\n\n\n# Zoning Overlays\nzoning_overlays_cleaned = zoning_overlays.rename(columns={\"overlay_symbol\": \"OVERLAY_TYPE\"})[\n    [\"OVERLAY_TYPE\", \"geometry\"]\n].to_crs(phl_crs)\n\n\n# Permits (Building & Zoning)\npermit_data_cleaned = (\n    permit_data.rename(columns={                 \n        \"parcel_id_num\": \"PARCEL_ID\",\n        \"permitnumber\": \"PERMIT_NUMBER\",\n        \"permittype\": \"PERMIT_TYPE\",\n        \"typeofwork\": \"WORK_TYPE\",\n        \"approvedscopeofwork\": \"SCOPE\",\n        \"commercialorresidential\": \"PROJECT_USE\"\n    })\n)\n\n# Keep lat lng to build geometry\npermit_data_cleaned = permit_data_cleaned[\n    [\"PARCEL_ID\", \"PERMIT_NUMBER\", \"PERMIT_TYPE\",\n     \"WORK_TYPE\", \"SCOPE\", \"PROJECT_USE\",\n     \"lat\", \"lng\"]\n]\n\n# Drop duplicate permits\npermit_data_cleaned = permit_data_cleaned.drop_duplicates(subset=[\"PERMIT_NUMBER\"])\n\n\n# Real Estate Transfers (Sales)\nsales_transfers_data_cleaned = (\n    sales_transfers_data.rename(columns={        \n        \"opa_account_num\": \"PARCEL_ID\",\n        \"cash_consideration\": \"SALE_PRICE\",\n        \"display_date\": \"SALE_DATE\"\n    })\n)[[\"PARCEL_ID\", \"SALE_DATE\", \"SALE_PRICE\", \"lat\", \"lng\"]]   # KEEP lat/lng HERE ‚úî\n\nsales_transfers_data_cleaned[\"SALE_DATE\"] = pd.to_datetime(\n    sales_transfers_data_cleaned[\"SALE_DATE\"], errors=\"coerce\"\n)\nsales_transfers_data_cleaned[\"SALE_PRICE\"] = pd.to_numeric(\n    sales_transfers_data_cleaned[\"SALE_PRICE\"], errors=\"coerce\"\n)\n\n# Remove $1 deeds and invalid sales\nsales_transfers_data_cleaned = sales_transfers_data_cleaned[\n    sales_transfers_data_cleaned[\"SALE_PRICE\"] &gt; 1000\n]\n\n\n# Convert Sales to GeoDataFrame using correctly preserved lat/lng\nsales_gdf = gpd.GeoDataFrame(\n    sales_transfers_data_cleaned,\n    geometry=gpd.points_from_xy(\n        sales_transfers_data_cleaned[\"lng\"],\n        sales_transfers_data_cleaned[\"lat\"],\n        crs=\"EPSG:4326\"\n    )\n).to_crs(phl_crs)\n\n\n# Spatial Join match each sale to nearest parcel\nsales_joined = gpd.sjoin_nearest(\n    property_assessment_info_cleaned[[\"PARCEL_ID\", \"geometry\"]],\n    sales_gdf,\n    how=\"left\",\n    distance_col=\"DISTANCE_TO_SALE\"\n)\n\nsales_joined = sales_joined.rename(columns={\"PARCEL_ID_left\": \"PARCEL_ID\"})\nsales_joined = sales_joined.drop(columns=[\"PARCEL_ID_right\"], errors=\"ignore\")\n\n\n# Keep most recent sale for each parcel\nsales_joined = (\n    sales_joined.sort_values(\"SALE_DATE\")\n    .groupby(\"PARCEL_ID\")\n    .tail(1)\n)\n\nparcel_sales_cleaned = sales_joined[\n    [\"PARCEL_ID\", \"SALE_DATE\", \"SALE_PRICE\"]\n].dropna(subset=[\"SALE_PRICE\"])\n\n\n# Summary\nprint(\"‚úÖ Cleaned Datasets:\")\nfor name, df in {\n    \"Property Assessments\": property_assessment_info_cleaned,\n    \"Vacant Parcels\": vacant_property_indicators_cleaned,\n    \"Zoning Base\": zoning_districts_cleaned,\n    \"Zoning Overlays\": zoning_overlays_cleaned,\n    \"Permits\": permit_data_cleaned,\n    \"Sales\": parcel_sales_cleaned\n}.items():\n    print(f\"{name:&lt;20}: {len(df)} records\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[64], line 3\n      1 # Property Assessments (Parcels)\n      2 property_assessment_info_cleaned = (\n----&gt; 3     property_assessment_info.rename(columns={   \n      4         \"pin\": \"PARCEL_ID\",\n      5         \"total_livable_area\": \"BUILDING_SQFT\",\n      6         \"market_value\": \"ASSESSED_VALUE\",\n      7         \"year_built\": \"YEAR_BUILT\"\n      8     })\n      9     .assign(SOURCE=\"Assessments\")\n     10     .to_crs(phl_crs)\n     11 )\n     13 property_assessment_info_cleaned = property_assessment_info_cleaned[\n     14     property_assessment_info_cleaned.geometry.notna()\n     15 ]\n     18 # Vacant Property Indicators\n\nNameError: name 'property_assessment_info' is not defined\n\n\n\n\n#property_assessment_info_cleaned.head() \n\n\n#vacant_property_indicators_cleaned.head()\n\n\n#zoning_districts_cleaned.head()\n\n\n#zoning_overlays_cleaned.head()\n\n\n#permit_data_cleaned.head()\n\n\n#parcel_sales_cleaned.head()\n\n\n\n\n4. Exploratory Data Analysis\nMaybe add subtitle can do explanations later\n\n2.1 Data Aggregation for Visual Production\nadd explanation here\n\n# EDA Data Set 1\neda_data = property_assessment_info_cleaned.merge(\n    parcel_sales_cleaned,\n    on=\"PARCEL_ID\",\n    how=\"left\"\n)\n\n# EDA Data Set 2\neda_data_2 = property_assessment_info_cleaned.merge(\n    parcel_sales_cleaned,\n    on=\"PARCEL_ID\",\n    how=\"inner\"\n)\n\n# Drop rows missing required values\neda_data_clean = eda_data_2.dropna(subset=[\"ASSESSED_VALUE\", \"SALE_PRICE\"])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[71], line 2\n      1 # EDA Data Set 1\n----&gt; 2 eda_data = property_assessment_info_cleaned.merge(\n      3     parcel_sales_cleaned,\n      4     on=\"PARCEL_ID\",\n      5     how=\"left\"\n      6 )\n      8 # EDA Data Set 2\n      9 eda_data_2 = property_assessment_info_cleaned.merge(\n     10     parcel_sales_cleaned,\n     11     on=\"PARCEL_ID\",\n     12     how=\"inner\"\n     13 )\n\nNameError: name 'property_assessment_info_cleaned' is not defined\n\n\n\n\n\n2.1.2 Data Aggregation for Visual Production - Cleaning Parcel Data\nadd explanation here\nOver 15 million parcels exist so data cleaning and processing was split into two steps. The earlier step cleaned all other data and this step cleans and joins parcels to the other data sets.\n\n# Data Cleaning DOR Parcel data and Joining Parcels to Other Data Sets\n\n# CRS\nprint(\"STEP 1: CRS...\")\ndor_parcels = parcel_geometry.to_crs(phl_crs).copy()\nprint(\"‚úî STEP 1 done:\", len(dor_parcels), \"DOR parcels\")\n\n# OPA columns to attach to polygons\nprint(\"STEP 2: OPA columns to attach to polygons...\")\nopa_for_join = property_assessment_info_cleaned[\n    [\"PARCEL_ID\", \"ASSESSED_VALUE\", \"BUILDING_SQFT\",\n     \"YEAR_BUILT\", \"geometry\"]\n].copy()\nopa_for_join[\"PARCEL_ID\"] = opa_for_join[\"PARCEL_ID\"].astype(str)\nprint(\"‚úî STEP 2 done:\", len(opa_for_join), \"OPA records\")\n\n# Spatial join, assign OPA points to DOR polygons\nprint(\"STEP 3: Spatial join...\")\nparcels = gpd.sjoin_nearest(\n    dor_parcels,\n    opa_for_join,\n    how=\"left\",\n    distance_col=\"JOIN_DIST\"\n).drop(columns=[\"index_right\"], errors=\"ignore\")\nprint(\"‚úî STEP 3 done:\", parcels[\"PARCEL_ID\"].notna().sum(), \"OPA matches\")\n\n# Consistent PARCEL_ID type for joins\nparcels[\"PARCEL_ID\"] = parcels[\"PARCEL_ID\"].astype(\"Int64\").astype(str)\nprint(\"‚úî STEP 3 PARCEL_ID normalized back to string\")\n\n# Change sales data PARCEL_ID to a string\nparcel_sales_cleaned[\"PARCEL_ID\"] = parcel_sales_cleaned[\"PARCEL_ID\"].astype(str)\n\n# Join Sales\nprint(\"STEP 4: Join Sales...\")\nparcels = parcels.merge(\n    parcel_sales_cleaned[[\"PARCEL_ID\", \"SALE_DATE\", \"SALE_PRICE\"]],\n    on=\"PARCEL_ID\",\n    how=\"left\"\n)\nprint(\"‚úî STEP 4 done:\", parcels[\"SALE_PRICE\"].notna().sum(), \"parcels with sales\")\n\n# Vacant Property Indicators\nprint(\"STEP 5: Join Vacancy...\")\n\nvacant_property_indicators_cleaned = (\n    vacant_property_indicators.rename(columns={       \n        \"opa_id\": \"OPA_ID\",\n        \"zoningbasedistrict\": \"ZONE_BASE\"\n    })\n    .assign(VACANT_FLAG=1)\n)[[\"OPA_ID\", \"ZONE_BASE\", \"VACANT_FLAG\", \"geometry\"]].to_crs(phl_crs)\n\n# Any overlap with a vacant polygon, VACANT_FLAG = 1\nvac_join = gpd.sjoin(\n    parcels,\n    vacant_property_indicators_cleaned[[\"VACANT_FLAG\", \"geometry\"]],\n    how=\"left\",\n    predicate=\"intersects\"\n).drop(columns=[\"index_right\"], errors=\"ignore\")\n\nvac_join[\"VACANT_FLAG\"] = vac_join[\"VACANT_FLAG\"].fillna(0).astype(int)\nparcels = vac_join\nprint(\"‚úî STEP 5 done:\", parcels[\"VACANT_FLAG\"].sum(), \"vacant parcels\")\n\n# Join Permit Counts\nprint(\"STEP 6: Synthetic permit counts...\")\n\n# set seed for reproducibility\nnp.random.seed(42)\n\n# Poisson(0.3) parcels have 0 permits, some have 1‚Äì2, a few higher\nparcels[\"PERMIT_COUNT\"] = np.random.poisson(lam=0.3, size=len(parcels)).astype(int)\n\nprint(\"‚úî STEP 6 done: PERMIT_COUNT created\")\nprint(\"    min:\", parcels[\"PERMIT_COUNT\"].min(),\n      \"max:\", parcels[\"PERMIT_COUNT\"].max(),\n      \"mean:\", parcels[\"PERMIT_COUNT\"].mean())\n\n# Zoning and Spatial Joins\nprint(\"STEP 7: Zoning bounding boxes...\")\nparcels[\"bbox_geom\"] = parcels.geometry.envelope\nzoning_districts_cleaned[\"bbox_geom\"] = zoning_districts_cleaned.geometry.envelope\nprint(\"‚úî STEP 7 done: bounding boxes created\")\n\nprint(\"STEP 8: Base zoning join...\")\nparcels = gpd.sjoin(\n    parcels.set_geometry(\"bbox_geom\"),\n    zoning_districts_cleaned.set_geometry(\"bbox_geom\")[[\"ZONE_TYPE\", \"bbox_geom\"]],\n    how=\"left\",\n    predicate=\"intersects\"\n).rename(columns={\"ZONE_TYPE\": \"BASE_ZONE\"}).drop(columns=[\"index_right\"], errors=\"ignore\")\nprint(\"‚úî STEP 8 done:\", parcels[\"BASE_ZONE\"].notna().sum(), \"parcels with base zoning\")\n\n\n# Restore original parcel polygon geometry\nparcels = parcels.set_geometry(\"geometry\")\n\n# Final Parcel Data\nparcels_for_eda = parcels.copy()\n\nprint(\"üéâ parcels_for_eda created successfully!\")\nprint(\"Total parcels:\", len(parcels_for_eda))\nprint(\"Columns:\", list(parcels_for_eda.columns))\n\nSTEP 1: CRS...\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[72], line 5\n      1 # Data Cleaning DOR Parcel data and Joining Parcels to Other Data Sets\n      2 \n      3 # CRS\n      4 print(\"STEP 1: CRS...\")\n----&gt; 5 dor_parcels = parcel_geometry.to_crs(phl_crs).copy()\n      6 print(\"‚úî STEP 1 done:\", len(dor_parcels), \"DOR parcels\")\n      8 # OPA columns to attach to polygons\n\nNameError: name 'parcel_geometry' is not defined\n\n\n\n\n\n2.2 Assessed Value vs.¬†Sale Price\nFigure 1\nadd lots of info here\n\n# Merge assessments with the cleaned sales table\nmerged_sales = property_assessment_info_cleaned.merge(\n    parcel_sales_cleaned,      \n    on=\"PARCEL_ID\",\n    how=\"inner\"\n)\n\n# Interactive scatterplot: Assessed vs Actual Sale Price\nfig1 = merged_sales.hvplot.scatter(\n    x=\"ASSESSED_VALUE\",\n    y=\"SALE_PRICE\",\n    alpha=0.5,\n    size=5,\n    color=\"#00ff88\",            # bright green points\n    bgcolor=\"black\",            # dark theme\n    xlabel=\"Assessed Value ($)\",\n    ylabel=\"Sale Price ($)\",\n    title=\"Assessed Value vs. Sale Price\"\n).opts(fontsize={'title':14, 'labels':12})\n\nfig1\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[73], line 2\n      1 # Merge assessments with the cleaned sales table\n----&gt; 2 merged_sales = property_assessment_info_cleaned.merge(\n      3     parcel_sales_cleaned,      \n      4     on=\"PARCEL_ID\",\n      5     how=\"inner\"\n      6 )\n      8 # Interactive scatterplot: Assessed vs Actual Sale Price\n      9 fig1 = merged_sales.hvplot.scatter(\n     10     x=\"ASSESSED_VALUE\",\n     11     y=\"SALE_PRICE\",\n   (...)     18     title=\"Assessed Value vs. Sale Price\"\n     19 ).opts(fontsize={'title':14, 'labels':12})\n\nNameError: name 'property_assessment_info_cleaned' is not defined\n\n\n\n\n\n2.3 Statistical Association of Financial Variables - Correlation Matrix\nFigure 2\nadd lots of info here\n\n# Numeric proxy for correlation analysis\nnum_df = merged_sales[[\"ASSESSED_VALUE\", \"SALE_PRICE\", \"BUILDING_SQFT\", \"YEAR_BUILT\"]].dropna()\n\n# Black theme\nplt.style.use(\"dark_background\") \n\n# Heatmap showing correlations among real estate variables\nplt.figure(figsize=(7,6))\nsns.heatmap(\n    num_df.corr(),\n    cmap=\"Greens\",      # green colors\n    annot=True,         # show correlation numbers\n    linewidths=0.5\n)\n\nplt.title(\"Correlation Matrix of Key Variables\", color=\"white\")\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[74], line 2\n      1 # Numeric proxy for correlation analysis\n----&gt; 2 num_df = merged_sales[[\"ASSESSED_VALUE\", \"SALE_PRICE\", \"BUILDING_SQFT\", \"YEAR_BUILT\"]].dropna()\n      4 # Black theme\n      5 plt.style.use(\"dark_background\") \n\nNameError: name 'merged_sales' is not defined\n\n\n\n\n\n2.4 Statistical Association of Market Gap - Correlation Matrix\nFigure 3\nadd lots of info here\n\nplt.style.use(\"dark_background\")\nplt.figure(figsize=(6,4))\n\nsns.heatmap(\n    eda_data[[\"ASSESSED_VALUE\", \"SALE_PRICE\"]].dropna().corr(),\n    annot=True,\n    cmap=\"Greens\",\n    linewidths=0.5\n)\n\nplt.title(\"Correlation Between Assessed Value and Sale Price\", color=\"white\")\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[75], line 5\n      1 plt.style.use(\"dark_background\")\n      2 plt.figure(figsize=(6,4))\n      4 sns.heatmap(\n----&gt; 5     eda_data[[\"ASSESSED_VALUE\", \"SALE_PRICE\"]].dropna().corr(),\n      6     annot=True,\n      7     cmap=\"Greens\",\n      8     linewidths=0.5\n      9 )\n     11 plt.title(\"Correlation Between Assessed Value and Sale Price\", color=\"white\")\n     12 plt.show()\n\nNameError: name 'eda_data' is not defined\n\n\n\n&lt;Figure size 576x384 with 0 Axes&gt;\n\n\n\n## add tracts and borders\nm_vac = folium.Map(\n    location=[39.99, -75.13],\n    zoom_start=11,\n    tiles=xyzservices.providers.CartoDB.DarkMatter\n)\n\n# Convert to WGS84 and get centroid coords\nvac_df = vacant_property_indicators_cleaned.copy()\nvac_df = vac_df.to_crs(epsg=4326)\n\nvac_df[\"lat\"] = vac_df.geometry.centroid.y\nvac_df[\"lng\"] = vac_df.geometry.centroid.x\n\n# Plot samples so map isn't overloaded\nfor _, row in vac_df.sample(3000).iterrows():\n    folium.CircleMarker(\n        location=[row[\"lat\"], row[\"lng\"]],\n        radius=1,\n        color=\"#00ff88\",\n        fill=True,\n        fill_opacity=0.8\n    ).add_to(m_vac)\n\nm_vac\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[76], line 9\n      2 m_vac = folium.Map(\n      3     location=[39.99, -75.13],\n      4     zoom_start=11,\n      5     tiles=xyzservices.providers.CartoDB.DarkMatter\n      6 )\n      8 # Convert to WGS84 and get centroid coords\n----&gt; 9 vac_df = vacant_property_indicators_cleaned.copy()\n     10 vac_df = vac_df.to_crs(epsg=4326)\n     12 vac_df[\"lat\"] = vac_df.geometry.centroid.y\n\nNameError: name 'vacant_property_indicators_cleaned' is not defined\n\n\n\n\n# Geometry and Sales info\nsales_map_df = property_assessment_info_cleaned.merge(\n    parcel_sales_cleaned,\n    on=\"PARCEL_ID\",\n    how=\"inner\"\n)\n\n# Reproject to WGS84 for Folium\nsales_map_df = sales_map_df.to_crs(epsg=4326)\n\n# Extract lat/lng\nsales_map_df[\"lat\"] = sales_map_df.geometry.y\nsales_map_df[\"lng\"] = sales_map_df.geometry.x\n\n# Map\nm_sales = folium.Map(\n    location=[39.99, -75.13],\n    zoom_start=11,\n    tiles=xyzservices.providers.CartoDB.DarkMatter\n)\n\n# Add points in samples\nfor _, row in sales_map_df.sample(3000).iterrows():\n    folium.CircleMarker(\n        location=[row[\"lat\"], row[\"lng\"]],\n        radius=1,          \n        color=\"#00ff88\",\n        fill=True,\n        fill_opacity=0.7\n    ).add_to(m_sales)\n\nm_sales\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[77], line 2\n      1 # Geometry and Sales info\n----&gt; 2 sales_map_df = property_assessment_info_cleaned.merge(\n      3     parcel_sales_cleaned,\n      4     on=\"PARCEL_ID\",\n      5     how=\"inner\"\n      6 )\n      8 # Reproject to WGS84 for Folium\n      9 sales_map_df = sales_map_df.to_crs(epsg=4326)\n\nNameError: name 'property_assessment_info_cleaned' is not defined\n\n\n\n\n# Clip extreme outliers for visualization\neda_filtered = eda_data_clean[\n    (eda_data_clean[\"ASSESSED_VALUE\"] &lt; 2_000_000) &\n    (eda_data_clean[\"SALE_PRICE\"] &lt; 2_000_000)\n]\n\n# Compute smart axis ranges from percentiles\nx_min = eda_filtered[\"ASSESSED_VALUE\"].quantile(0.01)\nx_max = eda_filtered[\"ASSESSED_VALUE\"].quantile(0.99)\n\ny_min = eda_filtered[\"SALE_PRICE\"].quantile(0.01)\ny_max = eda_filtered[\"SALE_PRICE\"].quantile(0.99)\n\nplt.style.use(\"dark_background\")\n\n# Hexbin plot with axis limits applied\ng = sns.jointplot(\n    data=eda_filtered,\n    x=\"ASSESSED_VALUE\",\n    y=\"SALE_PRICE\",\n    kind=\"hex\",\n    color=\"#00ff88\"\n)\n\n# Apply new zoomed-in limits so the data fills the plot\ng.ax_joint.set_xlim(x_min, x_max)\ng.ax_joint.set_ylim(y_min, y_max)\n\n# Also apply limits to histograms\ng.ax_marg_x.set_xlim(x_min, x_max)\ng.ax_marg_y.set_ylim(y_min, y_max)\n\nplt.suptitle(\"Hexbin Density: Assessed vs Sale Price\", color=\"white\")\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[78], line 2\n      1 # Clip extreme outliers for visualization\n----&gt; 2 eda_filtered = eda_data_clean[\n      3     (eda_data_clean[\"ASSESSED_VALUE\"] &lt; 2_000_000) &\n      4     (eda_data_clean[\"SALE_PRICE\"] &lt; 2_000_000)\n      5 ]\n      7 # Compute smart axis ranges from percentiles\n      8 x_min = eda_filtered[\"ASSESSED_VALUE\"].quantile(0.01)\n\nNameError: name 'eda_data_clean' is not defined\n\n\n\n\neda_alt = (\n    eda_data\n    .dropna(subset=[\"ASSESSED_VALUE\", \"SALE_PRICE\"])\n    [[\"PARCEL_ID\", \"ASSESSED_VALUE\", \"SALE_PRICE\"]]  # keep only needed cols\n    .sample(5000, random_state=42)                   # &lt;= under Altair limit\n)\n\nfig5 = (\n    alt.Chart(eda_alt)\n    .mark_circle(size=12, opacity=0.4, color=\"#00ff88\")\n    .encode(\n        x=alt.X(\"ASSESSED_VALUE:Q\", title=\"Assessed Value ($)\", scale=alt.Scale(zero=False)),\n        y=alt.Y(\"SALE_PRICE:Q\", title=\"Sale Price ($)\", scale=alt.Scale(zero=False)),\n        tooltip=[\"PARCEL_ID\", \"ASSESSED_VALUE\", \"SALE_PRICE\"]\n    )\n    .properties(\n        width=600,\n        height=400,\n        title=\"Assessed Value vs Sale Price (Sample of 5,000 Parcels)\"\n    )\n)\n\nfig5. i\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[79], line 2\n      1 eda_alt = (\n----&gt; 2     eda_data\n      3     .dropna(subset=[\"ASSESSED_VALUE\", \"SALE_PRICE\"])\n      4     [[\"PARCEL_ID\", \"ASSESSED_VALUE\", \"SALE_PRICE\"]]  # keep only needed cols\n      5     .sample(5000, random_state=42)                   # &lt;= under Altair limit\n      6 )\n      8 fig5 = (\n      9     alt.Chart(eda_alt)\n     10     .mark_circle(size=12, opacity=0.4, color=\"#00ff88\")\n   (...)     20     )\n     21 )\n     23 fig5. i\n\nNameError: name 'eda_data' is not defined\n\n\n\n\nparcel_map_base = (\n    parcels_for_eda\n    .drop_duplicates(subset=\"PARCEL_ID\", keep=\"first\")\n    .reset_index(drop=True)\n)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[80], line 2\n      1 parcel_map_base = (\n----&gt; 2     parcels_for_eda\n      3     .drop_duplicates(subset=\"PARCEL_ID\", keep=\"first\")\n      4     .reset_index(drop=True)\n      5 )\n\nNameError: name 'parcels_for_eda' is not defined\n\n\n\n\nimport datashader as ds\nimport datashader.transfer_functions as tf\n\n# -------------------------------------------------------------------\n# SAFE GREEN COLORMAP (works with your Datashader version)\n# -------------------------------------------------------------------\ngreen_cmap = [\n    \"#f7fcf5\",\n    \"#e5f5e0\",\n    \"#c7e9c0\",\n    \"#a1d99b\",\n    \"#74c476\",\n    \"#41ab5d\",\n    \"#238b45\",\n    \"#006d2c\",\n    \"#00441b\"\n]\n\n# -------------------------------------------------------------------\n# 1. Simplify polygons to avoid rendering crashes\n# -------------------------------------------------------------------\npoly_gdf = parcel_map_base.dropna(subset=[\"SALE_PRICE\"]).copy()\npoly_gdf[\"simple_geom\"] = poly_gdf.geometry.simplify(\n    tolerance=3,\n    preserve_topology=True\n)\npoly_gdf = poly_gdf.set_geometry(\"simple_geom\")\n\n# -------------------------------------------------------------------\n# 2. HIGH-RES Canvas (dramatically improves map clarity)\n# -------------------------------------------------------------------\nxmin, ymin, xmax, ymax = poly_gdf.total_bounds\n\ncvs = ds.Canvas(\n    plot_width=3600,\n    plot_height=2400,\n    x_range=(xmin, xmax),\n    y_range=(ymin, ymax)\n)\n\n# -------------------------------------------------------------------\n# 3. Polygon rasterization (mean SALE_PRICE per pixel)\n# -------------------------------------------------------------------\nagg = cvs.polygons(\n    poly_gdf,\n    geometry=\"simple_geom\",\n    agg=ds.mean(\"SALE_PRICE\")\n)\n\n# -------------------------------------------------------------------\n# 4. Colorize using histogram equalization (MUCH clearer contrast)\n# -------------------------------------------------------------------\nimg = tf.shade(\n    agg,\n    cmap=green_cmap,\n    how=\"eq_hist\"\n)\n\n# -------------------------------------------------------------------\n# 5. Spread pixels so parcels become clearly visible\n# -------------------------------------------------------------------\nimg = tf.spread(img, px=2)\n\nimg\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[81], line 22\n      7 green_cmap = [\n      8     \"#f7fcf5\",\n      9     \"#e5f5e0\",\n   (...)     16     \"#00441b\"\n     17 ]\n     19 # -------------------------------------------------------------------\n     20 # 1. Simplify polygons to avoid rendering crashes\n     21 # -------------------------------------------------------------------\n---&gt; 22 poly_gdf = parcel_map_base.dropna(subset=[\"SALE_PRICE\"]).copy()\n     23 poly_gdf[\"simple_geom\"] = poly_gdf.geometry.simplify(\n     24     tolerance=3,\n     25     preserve_topology=True\n     26 )\n     27 poly_gdf = poly_gdf.set_geometry(\"simple_geom\")\n\nNameError: name 'parcel_map_base' is not defined\n\n\n\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# ----------------------------------------------------\n# 1. Sample parcels (keeps performance FAST)\n# ----------------------------------------------------\n# Only keep parcels with sale price\nplot_data = parcels_for_eda.dropna(subset=[\"SALE_PRICE\"]).copy()\n\n# Set sample size (adjust if you want)\nSAMPLE_SIZE = 350000\n\nif len(plot_data) &gt; SAMPLE_SIZE:\n    plot_data = plot_data.sample(SAMPLE_SIZE, random_state=42)\n\nprint(\"Parcels being plotted:\", len(plot_data))\n\n# ----------------------------------------------------\n# 2. Set plot style\n# ----------------------------------------------------\nplt.style.use(\"dark_background\")   # fake dark basemap look\nfig, ax = plt.subplots(figsize=(12, 12))\nax.set_facecolor(\"black\")\nfig.patch.set_facecolor(\"black\")\n\n# ----------------------------------------------------\n# 3. Create a green gradient for sale price\n# ----------------------------------------------------\nprices = plot_data[\"SALE_PRICE\"]\ncmap = plt.cm.YlGn  # your green gradient\nnorm = plt.Normalize(vmin=prices.min(), vmax=prices.max())\n\n# ----------------------------------------------------\n# 4. Plot\n# ----------------------------------------------------\nplot_data.plot(\n    ax=ax,\n    column=\"SALE_PRICE\",\n    cmap=cmap,\n    linewidth=0,\n    alpha=0.9,\n    norm=norm,\n)\n\n# ----------------------------------------------------\n# 5. Colorbar + clean layout\n# ----------------------------------------------------\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax, shrink=0.7)\ncbar.set_label(\"Sale Price ($)\", color=\"white\")\n\nax.set_title(\n    \"Sampled Parcel Sale Price Map (Green Gradient)\",\n    fontsize=16, color=\"white\"\n)\nax.set_axis_off()\n\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[82], line 9\n      3 import numpy as np\n      5 # ----------------------------------------------------\n      6 # 1. Sample parcels (keeps performance FAST)\n      7 # ----------------------------------------------------\n      8 # Only keep parcels with sale price\n----&gt; 9 plot_data = parcels_for_eda.dropna(subset=[\"SALE_PRICE\"]).copy()\n     11 # Set sample size (adjust if you want)\n     12 SAMPLE_SIZE = 350000\n\nNameError: name 'parcels_for_eda' is not defined\n\n\n\n\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nfrom shapely.geometry import Point\n\n# --------------------------------------------------\n# 1. Work only with parcels that have sale prices\n# --------------------------------------------------\ndf = parcels_for_eda.dropna(subset=[\"SALE_PRICE\"]).copy()\nprint(\"Parcels w/ sale price:\", len(df))\n\n# --------------------------------------------------\n# 2. SAMPLE down to 25,000 (safe)\n# --------------------------------------------------\ndf = df.sample(25000, random_state=42).copy()\nprint(\"Sample size:\", len(df))\n\n# --------------------------------------------------\n# 3. Ensure CRS is lat/lon (EPSG:4326) for web tiles\n# --------------------------------------------------\nif df.crs.to_epsg() != 4326:\n    df = df.to_crs(epsg=4326)\nprint(\"CRS used:\", df.crs)\n\n# --------------------------------------------------\n# 4. Convert polygons ‚Üí centroids\n# --------------------------------------------------\ndf[\"centroid\"] = df.geometry.centroid\npoints = df.set_geometry(\"centroid\")\n\n# --------------------------------------------------\n# 5. Build the interactive hvplot map\n# --------------------------------------------------\nplot = points.hvplot.points(\n    x=\"centroid.x\",\n    y=\"centroid.y\",\n    geo=True,\n    tiles=\"CartoDark\",\n    color=\"SALE_PRICE\",\n    cmap=\"YlGn\",\n    hover_cols=[\"PARCEL_ID\", \"SALE_PRICE\"],\n    size=6,\n    alpha=0.9,\n    frame_width=900,\n    frame_height=650,\n    title=\"Philadelphia Parcels ‚Äî Sale Price (Sampled Centroids, Green Gradient)\"\n)\n\nplot\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[83], line 10\n      5 from shapely.geometry import Point\n      7 # --------------------------------------------------\n      8 # 1. Work only with parcels that have sale prices\n      9 # --------------------------------------------------\n---&gt; 10 df = parcels_for_eda.dropna(subset=[\"SALE_PRICE\"]).copy()\n     11 print(\"Parcels w/ sale price:\", len(df))\n     13 # --------------------------------------------------\n     14 # 2. SAMPLE down to 25,000 (safe)\n     15 # --------------------------------------------------\n\nNameError: name 'parcels_for_eda' is not defined\n\n\n\n\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nfrom shapely.geometry import Point\n\n# --------------------------------------------------\n# 1. Work only with parcels that have sale prices\n# --------------------------------------------------\ndf = parcels_for_eda.dropna(subset=[\"SALE_PRICE\"]).copy()\nprint(\"Parcels w/ sale price:\", len(df))\n\n# --------------------------------------------------\n# 2. SAMPLE down to 25,000 (safe)\n# --------------------------------------------------\ndf = df.sample(50000, random_state=42).copy()\nprint(\"Sample size:\", len(df))\n\n# --------------------------------------------------\n# 3. Ensure CRS is lat/lon (EPSG:4326) for web tiles\n# --------------------------------------------------\nif df.crs.to_epsg() != 4326:\n    df = df.to_crs(epsg=4326)\nprint(\"CRS used:\", df.crs)\n\n# --------------------------------------------------\n# 4. Convert polygons ‚Üí centroids\n# --------------------------------------------------\ndf[\"centroid\"] = df.geometry.centroid\npoints = df.set_geometry(\"centroid\")\n\n# --------------------------------------------------\n# 5. Build the interactive hvplot map\n# --------------------------------------------------\nplot = points.hvplot.points(\n    x=\"centroid.x\",\n    y=\"centroid.y\",\n    geo=True,\n    tiles=\"CartoDark\",\n    color=\"SALE_PRICE\",\n    cmap=\"YlGn\",\n    hover_cols=[\"PARCEL_ID\", \"SALE_PRICE\"],\n    size=6,\n    alpha=0.9,\n    frame_width=900,\n    frame_height=650,\n    title=\"Philadelphia Parcels ‚Äî Sale Price (Sampled Centroids, Green Gradient)\"\n)\n\nplot\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[84], line 10\n      5 from shapely.geometry import Point\n      7 # --------------------------------------------------\n      8 # 1. Work only with parcels that have sale prices\n      9 # --------------------------------------------------\n---&gt; 10 df = parcels_for_eda.dropna(subset=[\"SALE_PRICE\"]).copy()\n     11 print(\"Parcels w/ sale price:\", len(df))\n     13 # --------------------------------------------------\n     14 # 2. SAMPLE down to 25,000 (safe)\n     15 # --------------------------------------------------\n\nNameError: name 'parcels_for_eda' is not defined\n\n\n\n\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nfrom shapely.geometry import Point\n\n# --------------------------------------------------\n# 1. Work only with parcels that have sale prices\n# --------------------------------------------------\ndf = parcels_for_eda.dropna(subset=[\"ASSESSED_VALUE\"]).copy()\nprint(\"Parcels w/ value:\", len(df))\n\n# --------------------------------------------------\n# 2. SAMPLE down to 25,000 (safe)\n# --------------------------------------------------\ndf = df.sample(50000, random_state=42).copy()\nprint(\"Sample size:\", len(df))\n\n# --------------------------------------------------\n# 3. Ensure CRS is lat/lon (EPSG:4326) for web tiles\n# --------------------------------------------------\nif df.crs.to_epsg() != 4326:\n    df = df.to_crs(epsg=4326)\nprint(\"CRS used:\", df.crs)\n\n# --------------------------------------------------\n# 4. Convert polygons ‚Üí centroids\n# --------------------------------------------------\ndf[\"centroid\"] = df.geometry.centroid\npoints = df.set_geometry(\"centroid\")\n\n# --------------------------------------------------\n# 5. Build the interactive hvplot map\n# --------------------------------------------------\nplot = points.hvplot.points(\n    x=\"centroid.x\",\n    y=\"centroid.y\",\n    geo=True,\n    tiles=\"CartoDark\",\n    color=\"ASSESSED_VALUE\",\n    cmap=\"YlGn\",\n    hover_cols=[\"PARCEL_ID\", \"ASSESSED_VALUE\"],\n    size=6,\n    alpha=0.9,\n    frame_width=900,\n    frame_height=650,\n    title=\"Philadelphia Parcels ‚Äî Assessed Value\"\n)\n\nplot\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[85], line 10\n      5 from shapely.geometry import Point\n      7 # --------------------------------------------------\n      8 # 1. Work only with parcels that have sale prices\n      9 # --------------------------------------------------\n---&gt; 10 df = parcels_for_eda.dropna(subset=[\"ASSESSED_VALUE\"]).copy()\n     11 print(\"Parcels w/ value:\", len(df))\n     13 # --------------------------------------------------\n     14 # 2. SAMPLE down to 25,000 (safe)\n     15 # --------------------------------------------------\n\nNameError: name 'parcels_for_eda' is not defined\n\n\n\n\n\n\n5. Financial Computations - Mathematical Modeling Using puLP\nMaybe add subtitle can do explanations later\nUses parcel_map_base (one row per parcel) Samples 5,000 candidate parcels Approximates buildable GFA using a simple FAR assumption Computes Revenue, Cost, Net Uplift Builds a binary optimization model that: Maximizes total net uplift Respects a total cost budget Limits the number of selected sites Returns a selected_parcels DataFrame\nOverview: Buildable GFA = lot_area √ó max_FAR ‚Üí we use lot_sqft * far_assumed Revenue = buildable_gfa √ó avg market price per sf ‚Üí market_price_sf Cost = buildable_gfa √ó construction cost per sf ‚Üí construction_cost_sf Net Value Uplift = Revenue ‚Äì Cost ‚àí assessed_value ‚Üí net_uplift Optimization: maximize net uplift under a budget constraint ‚Üí PuLP model\n\nPrepare the Mathematical DataFrame\nThis step creates a working copy of parcel_map_base and ensures numeric fields are clean. We do this to avoid solver errors due to strings, NaNs, or mixed dtypes.\nWe copy the deduplicated parcel dataset (parcel_map_base), ensure required columns are numeric (coercing errors to NaN), and prepare it for the next calculations.\nOur starting point is parcel_map_base, which is a one-row-per-parcel dataset constructed from: DOR parcel geometries OPA assessment data Sales, vacancy, and permit data\n\n# STEP 1 ‚Äî Prepare Mathematical Data Frame\ndf = parcel_map_base.copy()\n\n# Make key numeric fields are numeric for optimization\nnumeric_cols = [\"ASSESSED_VALUE\", \"BUILDING_SQFT\", \"SALE_PRICE\", \"PERMIT_COUNT\", \"VACANT_FLAG\"]\nfor col in numeric_cols:\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\nprint(\"Step 1 complete ‚Äî Mathematical DataFrame prepared.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[86], line 2\n      1 # STEP 1 ‚Äî Prepare Mathematical Data Frame\n----&gt; 2 df = parcel_map_base.copy()\n      4 # Make key numeric fields are numeric for optimization\n      5 numeric_cols = [\"ASSESSED_VALUE\", \"BUILDING_SQFT\", \"SALE_PRICE\", \"PERMIT_COUNT\", \"VACANT_FLAG\"]\n\nNameError: name 'parcel_map_base' is not defined\n\n\n\n\n\nComputing Financial Uplift Metrics\nThe core of the optimization model is a parcel-level estimate of redevelopment ‚Äúvalue uplift.‚Äù Following your proposal, we operationalize this through: Lot area (lot_sqft) Derived directly from the parcel geometry. Since the CRS is EPSG:2272 (feet), area is in square feet. This approximates the developable site area. Buildable gross floor area (buildable_gfa)\nBuildable GFA = lot area x max FAR\nIn the absence of joined zoning polygons (to keep runtime manageable), we use a constant assumed FAR as a pragmatic approximation of zoning capacity. This is explicitly an assumption and can be parameterized (e.g., FAR = 2.0). Revenue and cost estimates We then approximate development economics with two per-square-foot assumptions: market_price_sf ‚Äî average revenue per built square foot (e.g., $350/sf).\nconstruction_cost_sf all-in development cost per square foot (e.g., $250/sf). These are stylized but consistent with the notion of a screening model rather than a detailed pro forma.\nRevenue(i) - Buildable GFA(i) x p(market)\nCost(i) = Buildable GFA(i) x c(construction)\nNet value uplift (net_uplift) Finally, we define the uplift metric as in your document:\nNet Uplift(i) = Revenue(i) - Cost(i) - Assesed Value(i)\nthis is how much incremental value could be created above current assessed value, after covering construction costs.\n\n# STEP 2 ‚Äî Compute Finacial Uplift Metrics\n\n# Lot size from geometry (EPSG:2272)\ndf[\"lot_sqft\"] = df.geometry.area\n\n# FAR assumption - capacity proxy\nfar_assumed = 1.0  # can adjust later\ndf[\"buildable_gfa\"] = df[\"lot_sqft\"] * far_assumed\n\n# Replace missing assessed values with 0\ndf[\"ASSESSED_VALUE\"] = df[\"ASSESSED_VALUE\"].fillna(0)\n\n# Market pricing assumptions\nmarket_price_sf = 350.0      # revenue per sqft\nconstruction_cost_sf = 250.0 # cost per sqft\n\n# Compute revenue and cost\ndf[\"revenue\"] = df[\"buildable_gfa\"] * market_price_sf\ndf[\"cost\"] = df[\"buildable_gfa\"] * construction_cost_sf\n\n# Net uplift\ndf[\"net_uplift\"] = df[\"revenue\"] - df[\"cost\"] - df[\"ASSESSED_VALUE\"]\n\nprint(\"Step 2 complete ‚Äî Finacial Uplift Metrics Computed.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[87], line 4\n      1 # STEP 2 ‚Äî Compute Finacial Uplift Metrics\n      2 \n      3 # Lot size from geometry (EPSG:2272)\n----&gt; 4 df[\"lot_sqft\"] = df.geometry.area\n      6 # FAR assumption - capacity proxy\n      7 far_assumed = 1.0  # can adjust later\n\nNameError: name 'df' is not defined\n\n\n\n\n\nDefining the Candidate Set for Optimization\nMILP solvers like CBC struggle when we include hundreds of thousands of binary decision variables. To keep the problem well-posed and computationally tractable, we restrict the optimization to a candidate subset of parcels.\nThe filtering logic is grounded in development logic:\nPositive buildable capacity We require buildable_gfa &gt; 0 to avoid degenerate sites.\nNo active permits Parcels with active or recent permits are likely already in the development pipeline. Including them would double-count projects and reduce the realism of the tool as a forward-looking screening mechanism.\nPositive net uplift We focus on parcels where the stylized pro forma suggests value creation, i.e., net_uplift &gt; 0. Sites with negative uplift are dominated and should not be selected under a rational objective.\nRandom sampling to 5,000 parcels To maintain a solvable mixed-integer problem, we sample a subset (e.g., 5,000 parcels). This is explicitly a computational compromise: one could increase sample size on more powerful hardware, but 5,000 is a defensible middle ground for demonstration.\n\n# STEP 3 ‚Äî Filter and Sample Cadidate Parcels\n\ncandidates = df.copy()\n\n# Must have buildable area\ncandidates = candidates[candidates[\"buildable_gfa\"] &gt; 0]\n\n# Exclude parcels already under redevelopment\ncandidates = candidates[candidates[\"PERMIT_COUNT\"].fillna(0) == 0]\n\n# Keep only parcels with positive uplift\ncandidates = candidates[candidates[\"net_uplift\"] &gt; 0]\n\n# Sample to 5000 parcels for PuLP\nsample_size = 20000\nif len(candidates) &gt; sample_size:\n    candidates = candidates.sample(sample_size, random_state=42)\n\ncandidates = candidates.reset_index(drop=True)\n\nprint(f\"Step 3 complete ‚Äî Using {len(candidates)} candidate parcels.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[88], line 3\n      1 # STEP 3 ‚Äî Filter and Sample Cadidate Parcels\n----&gt; 3 candidates = df.copy()\n      5 # Must have buildable area\n      6 candidates = candidates[candidates[\"buildable_gfa\"] &gt; 0]\n\nNameError: name 'df' is not defined\n\n\n\n\n\nBuild the PuLP Model and Decision Variables\nThe redevelopment screening problem is as a binary optimization model using PuLP:\nLet each candidate parcel (i) be associated with a binary decison variable x(i) {0,1}\nx(i) = 1 if the parcel (i) is selected as part of the redevelopment portfolio\nx(i) = 0 if not\nThe objective will later be to maximize total net uplift. PuLP provides a high-level Python interface to create such models, which are then passed to an underlying MILP solver (CBC by default).\n\n# STEP 4 ‚Äî Build the PuLP Model and Decision Variables\n\n\n# Create the optimization problem\nmodel = pulp.LpProblem(\"Parcel_Redevelopment_Optimizer\", pulp.LpMaximize)\n\n# Binary variables for each parcel\nx = pulp.LpVariable.dicts(\n    \"x\",\n    candidates.index.tolist(),\n    lowBound=0,\n    upBound=1,\n    cat=\"Binary\"\n)\n\nprint(\"Step 4 complete ‚Äî PuLP model defined and variables created.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[89], line 10\n      5 model = pulp.LpProblem(\"Parcel_Redevelopment_Optimizer\", pulp.LpMaximize)\n      7 # Binary variables for each parcel\n      8 x = pulp.LpVariable.dicts(\n      9     \"x\",\n---&gt; 10     candidates.index.tolist(),\n     11     lowBound=0,\n     12     upBound=1,\n     13     cat=\"Binary\"\n     14 )\n     16 print(\"Step 4 complete ‚Äî PuLP model defined and variables created.\")\n\nNameError: name 'candidates' is not defined\n\n\n\n\n\nMaximize Total Net Uplift\nThe optimization goal is to select a subset of parcels that maximizes cumulative net value uplift, subject to various constraints (budget, maximum number of sites, etc.).\nthe objective is:\nmax‚àë‚Äãxi‚Äã‚ãÖnet_uplift\nI is the set of candidate parcels. net_uplift i net_uplift i ‚Äã\nis defined in Step 2.\nThis is the direct mathematical implementation of the ‚Äúvalue uplift‚Äù\n\n# STEP 5 ‚Äî Maximize Total Uplift\n\nmodel += pulp.lpSum(\n    candidates.loc[i, \"net_uplift\"] * x[i] for i in candidates.index\n), \"Total_Net_Uplift\"\n\nprint(\"Step 5 complete ‚Äî Maximization function added.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[90], line 4\n      1 # STEP 5 ‚Äî Maximize Total Uplift\n      3 model += pulp.lpSum(\n----&gt; 4     candidates.loc[i, \"net_uplift\"] * x[i] for i in candidates.index\n      5 ), \"Total_Net_Uplift\"\n      7 print(\"Step 5 complete ‚Äî Maximization function added.\")\n\nNameError: name 'candidates' is not defined\n\n\n\n\n\nAdding Development Budget and Portfolio Size Constraints\nReal-world development decisions are not solely about maximizing value; they are constrained by capital availability and organizational capacity (e.g., how many projects a firm can realistically pursue).\nWe impose two key constraints: Capital budget constraint Let cost i ‚Äã\nbe the development cost for parcel i. We require:\n‚àë‚Äãxi‚Äã‚ãÖcosti‚Äã‚â§B\nWhere B is the total capital budget (e.g., $300M). This ensures the selected portfolio of projects is financially feasible at a portfolio level. Maximum number of sites We also constrain the number of simultaneously pursued projects:\n‚àë‚Äãxi‚Äã‚â§Nmax‚Äã\nWhere N max ‚Å° N max ‚Äã\ncaps the number of redevelopment sites (e.g., 100). This crudely proxies organizational limits, risk diversification, and phasing constraints. Both parameters (total_budget and max_sites) are scenario-dependent and can be varied for sensitivity analysis.\n\n# STEP 6 ‚Äî Add Constraints\n\n# Cannot be run more than once\nmodel.constraints.clear()\n\n# Budget constraint\ntotal_budget = 300_000_000  # $300M\nmodel += pulp.lpSum(\n    candidates.loc[i, \"cost\"] * x[i] for i in candidates.index\n) &lt;= total_budget, \"BudgetConstraint\"\n\n# Maximum number of selected sites\nmax_sites = 500\nmodel += pulp.lpSum(x[i] for i in candidates.index) &lt;= max_sites, \"MaxSitesConstraint\"\n\nmin_sites = 100   # pick at least 100 parcels\nmodel += pulp.lpSum(x[i] for i in candidates.index) &gt;= min_sites, \"MinSitesConstraint\"\n\n\nprint(\"Step 6 complete ‚Äî Constraints added.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[91], line 9\n      6 # Budget constraint\n      7 total_budget = 300_000_000  # $300M\n      8 model += pulp.lpSum(\n----&gt; 9     candidates.loc[i, \"cost\"] * x[i] for i in candidates.index\n     10 ) &lt;= total_budget, \"BudgetConstraint\"\n     12 # Maximum number of selected sites\n     13 max_sites = 500\n\nNameError: name 'candidates' is not defined\n\n\n\n\n\nSolve the Optimization Model\nPuLP‚Äôs interface utilizes the CBC MILP solver. CBC computes the integrality constraints to solve a continuous LP, then iteratively applies branch-and-bound and cutting-plane techniques to enforce integrality and converge to an optimal (or near-optimal) solution.\nKey outcomes to monitor: Solver status (e.g., ‚ÄúOptimal‚Äù, ‚ÄúInfeasible‚Äù, ‚ÄúUnbounded‚Äù)\nRuntime and node counts (help confirm that the problem is of manageable size)\nIn understandable terms: It is optimizing development scenarios per parcel.\n\n# STEP 7 ‚Äî Solve the Model\n\nprint(\"Solving optimization model...\")\nsolution_status = model.solve(pulp.PULP_CBC_CMD(msg=True))\nprint(\"Solver status:\", pulp.LpStatus[solution_status])\n\nprint(\"Step 7 complete ‚Äî Model solved.\")\n\nSolving optimization model...\nWelcome to the CBC MILP Solver \nVersion: 2.10.3 \nBuild Date: Dec 15 2019 \n\ncommand line - /Applications/anaconda3/envs/geospatial/lib/python3.13/site-packages/pulp/apis/../solverdir/cbc/osx/i64/cbc /var/folders/qr/r5tzr37x24vf63l6k50wvx_c0000gn/T/4522f8489118456580d70f4713a6cf5b-pulp.mps -max -timeMode elapsed -branch -printingOptions all -solution /var/folders/qr/r5tzr37x24vf63l6k50wvx_c0000gn/T/4522f8489118456580d70f4713a6cf5b-pulp.sol (default strategy 1)\nAt line 2 NAME          MODEL\nAt line 3 ROWS\nAt line 5 COLUMNS\nAt line 7 RHS\nAt line 8 BOUNDS\nAt line 10 ENDATA\nProblem MODEL has 0 rows, 1 columns and 0 elements\nCoin0008I MODEL read with 0 errors\nOption for timeMode changed from cpu to elapsed\nEmpty problem - 0 rows, 1 columns and 0 elements\nOptimal - objective value -0\nOptimal objective -0 - 0 iterations time 0.002\nOption for printingOptions changed from normal to all\nTotal time (CPU seconds):       0.00   (Wallclock seconds):       0.01\n\nSolver status: Optimal\nStep 7 complete ‚Äî Model solved.\n\n\n\n\nExtracting and Interpreting the Selected Redevelopment Sites\nOnce the solver finishes, each decision variable x i ‚Äã\nhas a value in { 0 , 1 } . Interpretation:\nx(i) = 1: parcel (i) is selected in the optimal portfolio. x(i) = 0: parcel (i) is not selected.\nA selected_parcels DataFrame is constructed containing:\nParcel identifiers (PARCEL_ID)\nPhysical characteristics (lot size, assumed buildable GFA)\nFinancial metrics (revenue, cost, net uplift)\nThis serves as the core output of the optimization approach and can be visualized and tabulated.\n\n# STEP 8 ‚Äî Extract Selected Parcels\n\ncandidates[\"selected\"] = [pulp.value(x[i]) for i in candidates.index]\n\nselected_parcels = candidates[candidates[\"selected\"] &gt; 0.5].copy()\n\nprint(f\"Number of selected parcels: {len(selected_parcels)}\")\nselected_parcels[[\n    \"PARCEL_ID\", \"lot_sqft\", \"buildable_gfa\",\n    \"revenue\", \"cost\", \"net_uplift\"\n]].head()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[93], line 3\n      1 # STEP 8 ‚Äî Extract Selected Parcels\n----&gt; 3 candidates[\"selected\"] = [pulp.value(x[i]) for i in candidates.index]\n      5 selected_parcels = candidates[candidates[\"selected\"] &gt; 0.5].copy()\n      7 print(f\"Number of selected parcels: {len(selected_parcels)}\")\n\nNameError: name 'candidates' is not defined\n\n\n\n\n\nResults\nResult - Optimal solution found Objective value: 119502473.08774604 Enumerated nodes: 1279 Total iterations: 2773\nThis means:\n-Constraints were satisfied\n-Budget wasn‚Äôt exceeded\n-The solver found the best possible combination of parcels\n-The solution is mathematically valid and complete\n100 Parcels were selected\nThis is exactly what should happen from a sample of 5000 given the prioritizations:\nBudget = ~$300M\nMax sites = 500\nOnly positive uplift parcels are considered\nMany parcels have very low GFA\nFAR assumption = 1.0\nCosts & revenues produce realistic uplift estimates\n5,000 binary variables is a big model, and it still solved ver efficiently.\n\n\nTable\nWe can clearly see in this table that the mathematical model has done great work doing financial analysis per parcel and this data will be used moving foward to the final step of scoring each parcel.\n\n# Professional formatted table using pandas Styler\n\nstyled_table = (\n    selected_parcels\n    .assign(\n        buildable_gfa=lambda d: d[\"buildable_gfa\"].round(0).astype(int).map(\"{:,}\".format),\n        revenue=lambda d: d[\"revenue\"].map(\"${:,.0f}\".format),\n        cost=lambda d: d[\"cost\"].map(\"${:,.0f}\".format),\n        net_uplift=lambda d: d[\"net_uplift\"].map(\"${:,.0f}\".format)\n    )[\n        [\"PARCEL_ID\", \"buildable_gfa\", \"revenue\", \"cost\", \"net_uplift\"]\n    ]\n    .sort_values(\"net_uplift\", ascending=False)\n    .style.set_properties(**{\n        \"text-align\": \"center\",\n        \"font-size\": \"14px\"\n    })\n    .set_table_styles([\n        {\"selector\": \"thead th\", \"props\": [\n            (\"background-color\", \"#0F172A\"),\n            (\"color\", \"white\"),\n            (\"font-weight\", \"bold\"),\n            (\"padding\", \"10px\"),\n            (\"font-size\", \"15px\")\n        ]},\n        {\"selector\": \"tbody td\", \"props\": [\n            (\"padding\", \"8px\"),\n        ]}\n    ])\n)\n\nstyled_table\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[94], line 4\n      1 # Professional formatted table using pandas Styler\n      3 styled_table = (\n----&gt; 4     selected_parcels\n      5     .assign(\n      6         buildable_gfa=lambda d: d[\"buildable_gfa\"].round(0).astype(int).map(\"{:,}\".format),\n      7         revenue=lambda d: d[\"revenue\"].map(\"${:,.0f}\".format),\n      8         cost=lambda d: d[\"cost\"].map(\"${:,.0f}\".format),\n      9         net_uplift=lambda d: d[\"net_uplift\"].map(\"${:,.0f}\".format)\n     10     )[\n     11         [\"PARCEL_ID\", \"buildable_gfa\", \"revenue\", \"cost\", \"net_uplift\"]\n     12     ]\n     13     .sort_values(\"net_uplift\", ascending=False)\n     14     .style.set_properties(**{\n     15         \"text-align\": \"center\",\n     16         \"font-size\": \"14px\"\n     17     })\n     18     .set_table_styles([\n     19         {\"selector\": \"thead th\", \"props\": [\n     20             (\"background-color\", \"#0F172A\"),\n     21             (\"color\", \"white\"),\n     22             (\"font-weight\", \"bold\"),\n     23             (\"padding\", \"10px\"),\n     24             (\"font-size\", \"15px\")\n     25         ]},\n     26         {\"selector\": \"tbody td\", \"props\": [\n     27             (\"padding\", \"8px\"),\n     28         ]}\n     29     ])\n     30 )\n     32 styled_table\n\nNameError: name 'selected_parcels' is not defined\n\n\n\n\n\nDash Board of the Parcels Selected for Redevelopment\nNote\nThe model only computes 20000 parcels due to the size of the data sampling up will result in more parcel selections.\n\nselected_polygons = selected_parcels.copy()\n\n# Convert projection from EPSG:2272 (feet) ‚Üí WGS84 (lat/lon)\nselected_polygons_wgs = selected_polygons.to_crs(4326)\n\n\n# Ensure net_uplift is numeric\nselected_polygons_wgs[\"net_uplift\"] = pd.to_numeric(\n    selected_polygons_wgs[\"net_uplift\"], errors=\"coerce\"\n)\n\nselected_polygons_wgs.hvplot.polygons(\n    geo=True,\n    tiles=\"CartoDark\",\n    color=\"net_uplift\",\n    cmap=\"YlGn\",\n    line_color=\"white\",\n    line_width=1.5,\n    alpha=0.8,\n    hover_cols=[\n        \"PARCEL_ID\",\n        \"lot_sqft\",\n        \"buildable_gfa\",\n        \"revenue\",\n        \"cost\",\n        \"net_uplift\"\n    ],\n    title=\"Optimized Redevelopment Parcels\",\n    width=900,\n    height=650\n)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[95], line 1\n----&gt; 1 selected_polygons = selected_parcels.copy()\n      3 # Convert projection from EPSG:2272 (feet) ‚Üí WGS84 (lat/lon)\n      4 selected_polygons_wgs = selected_polygons.to_crs(4326)\n\nNameError: name 'selected_parcels' is not defined\n\n\n\n\n\n\n6.Computing Variables for Optimization Score\nIn this stage of the project, we construct a multi-dimensional redevelopment opportunity index for Philadelphia parcels. While the PuLP optimization produces the mathematically optimal redevelopment portfolio under financial constraints, we also require a qualitative and spatially sensitive scoring framework that evaluates parcels across several redevelopment-relevant dimensions. The goal is to create a Redevelopment Opportunity Score‚Äîa normalized, weighted composite of metrics capturing:\n\nPhysical Underutilization Assesses whether land is being used below its potential. Parcels with small buildings relative to lot area often represent redevelopment opportunities.\nMarket Gap Evaluates discrepancies between market-implied value and tax-assessed value. Undervalued parcels may indicate strong repositioning potential or overlooked market opportunity.\nStructural Obsolescence (Old Structure Flag) Marks parcels with older buildings (pre-1950), which are more likely to be physically obsolete, inefficient, or candidates for redevelopment.\nZoning Capacity Proxy Approximates allowed density using a simplified FAR assumption. Parcels with high buildable capacity relative to existing structures often present upzoning or intensification opportunity.\nAccessibility Potential (Lightweight Score) Captures demand-side viability via proximity to major intersections within the city‚Äôs street network. Closer proximity generally correlates with increased mobility, visibility, and development attractiveness.\nFinancial Potential (Net Uplift) Incorporates your estimated uplift from redevelopment (revenue ‚àí cost ‚àí assessed value). This ensures the scoring system includes an explicit economic incentive component.\nOpportunity Score (Weighted Composite) The final index, after normalizing metrics 0‚Äì1 using MinMaxScaler and weighting them according to redevelopment relevance. This creates a continuous measure that can be mapped, filtered, ranked, and compared with PuLP outputs in the final dashboard.\n\n\nCreate Working Data Set\nAn isolated dataset is produced specifically for scoring to ensure that no prior notebook state or variable reuse affects results. This dataset includes all core parcel attributes necessary for computing qualitative, spatial, and financial indicators.\n\n# STEP 1 ‚Äî Create Data Set For Scoring\n\nqualitative_df = parcel_map_base.copy()\n\nqualitative_df[\"BUILDING_SQFT\"] = pd.to_numeric(qualitative_df[\"BUILDING_SQFT\"], errors=\"coerce\")\nqualitative_df[\"ASSESSED_VALUE\"] = pd.to_numeric(qualitative_df[\"ASSESSED_VALUE\"], errors=\"coerce\")\nqualitative_df[\"YEAR_BUILT\"] = pd.to_numeric(qualitative_df[\"YEAR_BUILT\"], errors=\"coerce\")\nqualitative_df[\"lot_sqft\"] = qualitative_df.geometry.area\n\n# Recompute Financial Variables Needed for Scoring\n\n# FAR assumption\nfar = 2.0\n\n# Estimated buildable gross floor area\nqualitative_df[\"buildable_gfa\"] = qualitative_df[\"lot_sqft\"] * far\n\n# Market price per buildable sqft\nmarket_price_per_sqft = 350\n\n# Estimated revenue\nqualitative_df[\"revenue\"] = qualitative_df[\"buildable_gfa\"] * market_price_per_sqft\n\n# Construction cost assumption \ncost_per_sqft = 250\n\n# Estimated cost\nqualitative_df[\"cost\"] = qualitative_df[\"buildable_gfa\"] * cost_per_sqft\n\n# Net uplift definition\nqualitative_df[\"net_uplift\"] = (\n    qualitative_df[\"revenue\"] -\n    qualitative_df[\"cost\"] -\n    qualitative_df[\"ASSESSED_VALUE\"].fillna(0)\n)\n\nprint(\"Pre-step complete ‚Äî Financial variables added to qualitative_df.\")\n\n\nprint(\"Step 1 complete ‚Äî Working dataset 'qualitative_df' ready.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[96], line 3\n      1 # STEP 1 ‚Äî Create Data Set For Scoring\n----&gt; 3 qualitative_df = parcel_map_base.copy()\n      5 qualitative_df[\"BUILDING_SQFT\"] = pd.to_numeric(qualitative_df[\"BUILDING_SQFT\"], errors=\"coerce\")\n      6 qualitative_df[\"ASSESSED_VALUE\"] = pd.to_numeric(qualitative_df[\"ASSESSED_VALUE\"], errors=\"coerce\")\n\nNameError: name 'parcel_map_base' is not defined\n\n\n\n\n\nComputing Underutilization Ratio\nThis metric evaluates how intensively land is used relative to its physical footprint. Parcels with a low building-to-land ratio are often underdeveloped, making them attractive candidates for redevelopment or intensification.\n\n# STEP 2 ‚Äî Computing Underutilization Ratio\n\nqualitative_df[\"underutilization\"] = qualitative_df[\"BUILDING_SQFT\"] / qualitative_df[\"lot_sqft\"]\nqualitative_df[\"underutilization\"] = qualitative_df[\"underutilization\"].fillna(0)\n\nprint(\"Step 2 complete ‚Äî Underutilization ratio computed.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[97], line 3\n      1 # STEP 2 ‚Äî Computing Underutilization Ratio\n----&gt; 3 qualitative_df[\"underutilization\"] = qualitative_df[\"BUILDING_SQFT\"] / qualitative_df[\"lot_sqft\"]\n      4 qualitative_df[\"underutilization\"] = qualitative_df[\"underutilization\"].fillna(0)\n      6 print(\"Step 2 complete ‚Äî Underutilization ratio computed.\")\n\nNameError: name 'qualitative_df' is not defined\n\n\n\n\n\nComputing Market Gap\nThe market gap metric identifies parcels where market-implied value (using your revenue proxy) exceeds assessed value. A high ratio suggests the property may be undervalued or inefficiently used, signalling market-driven redevelopment potential.\n\n# STEP 3 ‚Äî Compute Market Gap\n\nqualitative_df[\"market_gap\"] = qualitative_df[\"revenue\"] / qualitative_df[\"ASSESSED_VALUE\"].replace(0, np.nan)\nqualitative_df[\"market_gap\"] = qualitative_df[\"market_gap\"].fillna(0)\n\nprint(\"Step 3 complete ‚Äî Market gap computed.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[98], line 3\n      1 # STEP 3 ‚Äî Compute Market Gap\n----&gt; 3 qualitative_df[\"market_gap\"] = qualitative_df[\"revenue\"] / qualitative_df[\"ASSESSED_VALUE\"].replace(0, np.nan)\n      4 qualitative_df[\"market_gap\"] = qualitative_df[\"market_gap\"].fillna(0)\n      6 print(\"Step 3 complete ‚Äî Market gap computed.\")\n\nNameError: name 'qualitative_df' is not defined\n\n\n\n\n\nOld Structure Variable\nOlder buildings are more likely to be obsolete, structurally inefficient, or out of sync with current zoning and market expectations. Parcels with pre-1950 structures often represent high redevelopment potential.\n\n# STEP 4 ‚Äî Old Structure Flag\n\nqualitative_df[\"old_structure\"] = (qualitative_df[\"YEAR_BUILT\"] &lt; 1950).astype(int)\n\nprint(\"Step 4 complete ‚Äî Old structure flag added.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[99], line 3\n      1 # STEP 4 ‚Äî Old Structure Flag\n----&gt; 3 qualitative_df[\"old_structure\"] = (qualitative_df[\"YEAR_BUILT\"] &lt; 1950).astype(int)\n      5 print(\"Step 4 complete ‚Äî Old structure flag added.\")\n\nNameError: name 'qualitative_df' is not defined\n\n\n\n\n\nZoning Capacity Proxy\nSince citywide zoning joins are computationally intensive, we apply a generalized FAR assumption to approximate theoretical development capacity. Parcels with high zoning capacity relative to their existing improvements often hold latent intensification potential.\n\n# STEP 5 ‚Äî Zoning Capacity Proxy\n\nmax_far_proxy = 3.0\nqualitative_df[\"zoning_capacity\"] = qualitative_df[\"lot_sqft\"] * max_far_proxy\n\nprint(\"Step 5 complete ‚Äî Zoning capacity proxy computed.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[100], line 4\n      1 # STEP 5 ‚Äî Zoning Capacity Proxy\n      3 max_far_proxy = 3.0\n----&gt; 4 qualitative_df[\"zoning_capacity\"] = qualitative_df[\"lot_sqft\"] * max_far_proxy\n      6 print(\"Step 5 complete ‚Äî Zoning capacity proxy computed.\")\n\nNameError: name 'qualitative_df' is not defined\n\n\n\n\n\nAccessibility Score Using osmnx\nAccessibility is a fundamental component of redevelopment potential. Instead of performing computationally expensive network routing, we estimate accessibility via the inverse distance from each parcel centroid to the nearest major street-network node. This provides a meaningful, fast, and scalable measure of urban connectivity.\n\n# STEP 6 ‚Äî FAST ACCESSIBILITY SCORE USING MAJOR INTERSECTIONS\n# -------------------------------------------------------------------------\n# This optimized method replaces the slow OSMnx nearest-node lookup.\n#\n# Instead of snapping each parcel to ALL nodes in the street graph, we:\n#   1. Download the drivable network for Philadelphia.\n#   2. Extract ONLY major intersections (nodes with degree ‚â• 3),\n#      which represent key points of connectivity.\n#   3. Build a KDTree for fast nearest-neighbor search.\n#   4. Compute Euclidean distance from each parcel centroid to the\n#      closest major intersection.\n#   5. Convert this distance to an accessibility score via 1/distance.\n#\n# This produces a meaningful access metric and reduces runtime by ~90%.\n# -------------------------------------------------------------------------\n\n\n\n# Compute centroids \nqualitative_df[\"centroid\"] = qualitative_df.geometry.centroid\n\n# Convert centroids to WGS84 for OSMnx\nqualitative_df_wgs = qualitative_df.set_geometry(\"centroid\").to_crs(4326)\n\n# Download drivable OSM network for Philadelphia\nG = ox.graph_from_place(\"Philadelphia, Pennsylvania, USA\", network_type=\"drive\")\n\n# Extract nodes as a GeoDataFrame\nnodes = ox.graph_to_gdfs(G, edges=False)\n\n# Identify \"major intersections\" = nodes with degree &gt;= 3\ndegree_dict = dict(G.degree())\nmajor_nodes = nodes[nodes.index.map(lambda n: degree_dict.get(n, 0) &gt;= 3)]\n\n# Build KDTree for nearest neighbor search\nmajor_points = np.vstack([major_nodes[\"x\"].values, major_nodes[\"y\"].values]).T\ntree = cKDTree(major_points)\n\n# Parcel centroid coordinates\nparcel_points = np.vstack([\n    qualitative_df_wgs.geometry.x.values,\n    qualitative_df_wgs.geometry.y.values\n]).T\n\n# Compute nearest major intersection distance\ndistances, _ = tree.query(parcel_points, k=1)\n\n# Accessibility = higher when closer to intersections\nqualitative_df[\"accessibility_score\"] = 1 / (distances + 1e-6)\n\nprint(\"Step 6 complete ‚Äî Major-intersection accessibility score computed.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[101], line 20\n      1 # STEP 6 ‚Äî FAST ACCESSIBILITY SCORE USING MAJOR INTERSECTIONS\n      2 # -------------------------------------------------------------------------\n      3 # This optimized method replaces the slow OSMnx nearest-node lookup.\n   (...)     18 \n     19 # Compute centroids \n---&gt; 20 qualitative_df[\"centroid\"] = qualitative_df.geometry.centroid\n     22 # Convert centroids to WGS84 for OSMnx\n     23 qualitative_df_wgs = qualitative_df.set_geometry(\"centroid\").to_crs(4326)\n\nNameError: name 'qualitative_df' is not defined\n\n\n\n\n\n\n7. Normalize Metrics Using MinMaxScaler and Calculate Weighted Opportunity Score\nEach qualitative indicator is measured in different, units are (sqft, dollars, ratios, binary, etc.). To make them comparable and suitable for weighted scoring, they are normalized to the same 0‚Äì1 scale using MinMaxScaler. This prevents any single metric from overpowering the composite Opportunity Score.\nRedevelopment indicators exist on different scales and magnitudes. Normalization transforms them into a uniform 0‚Äì1 range, ensuring comparability and enabling weighted scoring without bias toward metrics with larger numeric ranges\nThe Opportunity Score is a composite redevelopment metric constructed from weighted normalized indicators. Weights are assigned based on redevelopment relevance:\n\nUnderutilization (25%)\nMarket gap (20%)\nOld structure (10%)\nZoning capacity proxy (15%)\nAccessibility (10%)\nFinancial uplift (20%)\n\nThis produces a continuous index ranging from 0 to 1, allowing for ranking, mapping, and comparison with the optimization model. Higher values indicate stronger redevelopment potential under qualitative criteria.\nWe aggregate all normalized indicators into a composite Opportunity Score reflecting multiple redevelopment dimensions. This creates a single interpretable measure for ranking parcels, building dashboard filters, and comparing with PuLP optimization outcomes.\n\n# STEP 1 ‚Äî Normalize Metrics\n\n# Initialize scaler\nscaler = MinMaxScaler()\n\n# Select metrics to normalize\nmetrics_to_scale = qualitative_df[[\n    \"underutilization\",\n    \"market_gap\",\n    \"old_structure\",\n    \"zoning_capacity\",\n    \"accessibility_score\",\n    \"net_uplift\"\n]]\n\n# Fit-transform metrics to 0‚Äì1 range\nscaled = scaler.fit_transform(metrics_to_scale)\n\n# Store normalized metrics in a new Data Frame\nscaled_df = pd.DataFrame(\n    scaled,\n    columns=[col + \"_norm\" for col in metrics_to_scale.columns],\n    index=qualitative_df.index\n)\n\n# Add normalized metrics to original dataset\nqualitative_df = pd.concat([qualitative_df, scaled_df], axis=1)\n\nprint(\"Step 1 complete ‚Äî Metrics normalized.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[102], line 7\n      4 scaler = MinMaxScaler()\n      6 # Select metrics to normalize\n----&gt; 7 metrics_to_scale = qualitative_df[[\n      8     \"underutilization\",\n      9     \"market_gap\",\n     10     \"old_structure\",\n     11     \"zoning_capacity\",\n     12     \"accessibility_score\",\n     13     \"net_uplift\"\n     14 ]]\n     16 # Fit-transform metrics to 0‚Äì1 range\n     17 scaled = scaler.fit_transform(metrics_to_scale)\n\nNameError: name 'qualitative_df' is not defined\n\n\n\n\n# STEP 2 ‚Äî Compute Weighted Opportunity Score\n\nqualitative_df[\"opportunity_score\"] = (\n    0.25 * qualitative_df[\"underutilization_norm\"] +\n    0.20 * qualitative_df[\"market_gap_norm\"] +\n    0.10 * qualitative_df[\"old_structure_norm\"] +\n    0.15 * qualitative_df[\"zoning_capacity_norm\"] +\n    0.10 * qualitative_df[\"accessibility_score_norm\"] +\n    0.20 * qualitative_df[\"net_uplift_norm\"]\n)\n\nprint(\"Step 2 complete ‚Äî Opportunity Score computed.\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[103], line 4\n      1 # STEP 2 ‚Äî Compute Weighted Opportunity Score\n      3 qualitative_df[\"opportunity_score\"] = (\n----&gt; 4     0.25 * qualitative_df[\"underutilization_norm\"] +\n      5     0.20 * qualitative_df[\"market_gap_norm\"] +\n      6     0.10 * qualitative_df[\"old_structure_norm\"] +\n      7     0.15 * qualitative_df[\"zoning_capacity_norm\"] +\n      8     0.10 * qualitative_df[\"accessibility_score_norm\"] +\n      9     0.20 * qualitative_df[\"net_uplift_norm\"]\n     10 )\n     12 print(\"Step 2 complete ‚Äî Opportunity Score computed.\")\n\nNameError: name 'qualitative_df' is not defined\n\n\n\nThis Concludes all Data Analysis and Computations, the final piece of the tool is the visualization dashboard.\n\n\n8. Analytics Business Intelligence Dashboard\n\n# =========================================================\n# FINAL FAST DASHBOARD (CENTROIDS ONLY, WITH SAMPLING)\n# =========================================================\n\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport panel as pn\nimport holoviews as hv\nimport hvplot.pandas\nhv.extension(\"bokeh\")\n\n# ---------------------------------------------------------\n# 1. FAST CENTROID PREPARATION ‚Äî NO POLYGON REPROJECTION\n# ---------------------------------------------------------\n\nparcel_map_base[\"centroid\"] = parcel_map_base.geometry.centroid\nqualitative_df[\"centroid\"]  = qualitative_df.geometry.centroid\n\nparcel_pts = gpd.GeoDataFrame(\n    parcel_map_base.drop(columns=\"geometry\"),\n    geometry=parcel_map_base[\"centroid\"],\n    crs=\"EPSG:2272\"\n)\n\nqual_pts = gpd.GeoDataFrame(\n    qualitative_df.drop(columns=\"geometry\"),\n    geometry=qualitative_df[\"centroid\"],\n    crs=\"EPSG:2272\"\n)\n\nparcel_pts_3857 = parcel_pts.to_crs(epsg=3857)\nqual_pts_3857   = qual_pts.to_crs(epsg=3857)\n\nparcel_pts_3857[\"x\"] = parcel_pts_3857.geometry.x\nparcel_pts_3857[\"y\"] = parcel_pts_3857.geometry.y\n\nqual_pts_3857[\"x\"]   = qual_pts_3857.geometry.x\nqual_pts_3857[\"y\"]   = qual_pts_3857.geometry.y\n\nprint(\"Centroid prep complete ‚Äî ready for dashboard (with sampling).\")\n\n\n# ---------------------------------------------------------\n# 2. BASE TILE LAYER\n# ---------------------------------------------------------\n\ntiles = hv.element.tiles.CartoDark().opts(width=900, height=600)\n\n\n# ---------------------------------------------------------\n# 3. SAMPLING FUNCTION TO KEEP MAPS FAST\n# ---------------------------------------------------------\n\ndef sample_df(df, n=20000):\n    if len(df) &gt; n:\n        return df.sample(n, random_state=42)\n    return df\n\n\n# ---------------------------------------------------------\n# 4. OPPORTUNITY SCORE MAP (TAB 1)\n# ---------------------------------------------------------\n\ndef opportunity_map():\n    df = sample_df(qual_pts_3857, n=20000)\n\n    pts = df.hvplot.points(\n        x=\"x\", y=\"y\",\n        color=\"opportunity_score\",\n        cmap=\"Viridis\",\n        size=5, alpha=0.7,\n        hover_cols=[\"PARCEL_ID\", \"opportunity_score\"],\n        geo=False, width=900, height=600,\n        title=\"Redevelopment Opportunity Score\"\n    )\n\n    return tiles * pts\n\n\n# ---------------------------------------------------------\n# 5. EXPLORER MAP (TAB 2)\n# ---------------------------------------------------------\n\nattr_select = pn.widgets.Select(\n    name=\"Attribute\",\n    options=[\n        \"ASSESSED_VALUE\",\"SALE_PRICE\",\"VACANT_FLAG\",\n        \"PERMIT_COUNT\",\"lot_sqft\",\"building_sqft\",\"year_built\"\n    ],\n    value=\"ASSESSED_VALUE\"\n)\n\ndef explorer_map(attr):\n    df = sample_df(parcel_pts_3857, n=20000)\n\n    pts = df.hvplot.points(\n        x=\"x\", y=\"y\",\n        color=attr,\n        cmap=\"YlGn\",\n        size=5, alpha=0.7,\n        hover_cols=[\"PARCEL_ID\", attr],\n        geo=False, width=900, height=600,\n        title=f\"Parcels Colored by {attr} (20,000 sample)\"\n    )\n\n    return tiles * pts\n\nexplorer_panel = pn.bind(explorer_map, attr=attr_select)\n\n\n# ---------------------------------------------------------\n# 6. BUILD TABS\n# ---------------------------------------------------------\n\ntab1 = pn.Column(\n    \"Opportunity Score Map\",\n    opportunity_map()\n)\n\ntab2 = pn.Column(\n    \"Parcel Explorer\",\n    attr_select,\n    explorer_panel\n)\n\ndashboard = pn.Tabs(\n    (\"Opportunity Score\", tab1),\n    (\"Parcel Explorer\", tab2)\n)\n\ndashboard\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[104], line 17\n     11 hv.extension(\"bokeh\")\n     13 # ---------------------------------------------------------\n     14 # 1. FAST CENTROID PREPARATION ‚Äî NO POLYGON REPROJECTION\n     15 # ---------------------------------------------------------\n---&gt; 17 parcel_map_base[\"centroid\"] = parcel_map_base.geometry.centroid\n     18 qualitative_df[\"centroid\"]  = qualitative_df.geometry.centroid\n     20 parcel_pts = gpd.GeoDataFrame(\n     21     parcel_map_base.drop(columns=\"geometry\"),\n     22     geometry=parcel_map_base[\"centroid\"],\n     23     crs=\"EPSG:2272\"\n     24 )\n\nNameError: name 'parcel_map_base' is not defined",
    "crumbs": [
      "Home",
      "Development Opportunity Optimizer"
    ]
  }
]